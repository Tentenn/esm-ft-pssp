# For more options, check out:
# https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_command_ref/bsub.yaml.1.html
io: 
    outputOverwriteFile: stdout.log
    errorOverwriteFile: stderr.log
limit: 
    coreLimit: 16
    # in hh:mm
    runtimeLimit: 2400:00
    # Limit the execution to 8GB of CPU RAM
    memLimit: 128G!
resource: 
    # GPU options
    # shared job up to 43GB of GPU RAM
    # IMPORTANT: limits are not strictly enforced
    # make sure you allocate as much as you will maximally need!
    # Failing to do so may result in your or someone elses job failing.
    # gpu: num=1/task:mode=shared:gmem=42G:j_exclusive=no:gpack=yes
    # If job>43GB, ask for exclusive GPU use
    # this MUST be limited to 2 exclusive use jobs per user!
    gpu: num=1:mode=exclusive_process:gmem=47G:j_exclusive=yes

    # Which machine to use: 
    #  - lsf-server-2 = CD/big
    #  - lsf-server-3 = titanx
    # To use any, uncomment! 
    machines: lsf-server-2


## Uncommment the following to schedule a job to start at a specific time
## In the following case, the job will be scheduled at 8PM of the day you submit it
## The format is YYYY:MM:DD:HH:M, https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_command_ref/bsub.b.1.html
# schedule:
#    specifiedStartTime: 20:00


## Uncomment the following to get job status updates on your @rostlab.org email
## Note: the default behaviour is that the @rostlab.org email forwards to your TUM email
#notify:
#    notifyJobDone: ""
#    notifyJobExit:
#    jobWaitDone:
#    notifyJobDispatch:


properties:
    queueName: mid-end-normal
    jobName: tang_make_esm_embeds_all_test
# command: bash hp_tests/mlm.sh
# command: python pssp_finetune_manual.py --datapath /mnt/project/tang/esm-ft-pssp/data/ --plm_checkpoint 8M --wdnote 8M && python pssp_finetune_manual.py --datapath /mnt/project/tang/esm-ft-pssp/data/ --plm_checkpoint 35M --wdnote 35M && python pssp_finetune_manual.py --datapath /mnt/project/tang/esm-ft-pssp/data/ --plm_checkpoint 150M --wdnote 150M && python pssp_finetune_manual.py --datapath /mnt/project/tang/esm-ft-pssp/data/ --plm_checkpoint 650M --wdnote 650M
# command: python pssp_finetune_manual.py --epochs 16 --bs 1 --plm_checkpoint 3B --wdnote 3B-PEFT-no-1000-10ep --peft_mode all --trainset train.jsonl
'''command:
    python make_esm_embeddings.py data/casp12.jsonl -1 data/embeddings 8M &&
    python make_esm_embeddings.py data/new_pisces.jsonl -1 data/embeddings 8M &&
    python make_esm_embeddings.py data/casp12.jsonl -1 data/embeddings 35M &&
    python make_esm_embeddings.py data/new_pisces.jsonl -1 data/embeddings 35M &&
    python make_esm_embeddings.py data/casp12.jsonl -1 data/embeddings 150M &&
    python make_esm_embeddings.py data/new_pisces.jsonl -1 data/embeddings 150M &&
    python make_esm_embeddings.py data/casp12.jsonl -1 data/embeddings 650M &&
    python make_esm_embeddings.py data/new_pisces.jsonl -1 data/embeddings 650M &&
    python make_esm_embeddings.py data/casp12.jsonl -1 data/embeddings 3B &&
    python make_esm_embeddings.py data/new_pisces.jsonl -1 data/embeddings 3B'''
command:
    python make_esm_embeddings.py data/train.jsonl -1 data/embeddings 150M &&
    python make_esm_embeddings.py data/val.jsonl -1 data/embeddings 150M

# tra-in
# t6_8M
# python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_cut256_esm2_t6_8M_UR50D_embeddings.h5 --vemb data/embeddings/val_no1000_esm2_t6_8M_UR50D_embeddings.h5 --casp12_embeds data/embeddings/10seqs_esm2_t6_8M_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/10seqs_esm2_t6_8M_UR50D_embeddings.h5 --train_labels data/10seqs.jsonl --val_labels data/10seqs.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl
# t