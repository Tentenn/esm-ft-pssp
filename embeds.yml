# For more options, check out:
# https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_command_ref/bsub.yaml.1.html
io:
    outputOverwriteFile: stdout.log
    errorOverwriteFile: stderr.log
limit:
    coreLimit: 8
    # in hh:mm
    runtimeLimit: 600:00
    # Limit the execution to 8GB of CPU RAM
    memLimit: 64G!
resource:
    # GPU options
    # shared job up to 43GB of GPU RAM
    # IMPORTANT: limits are not strictly enforced
    # make sure you allocate as much as you will maximally need!
    # Failing to do so may result in your or someone elses job failing.
    gpu: num=1/task:mode=shared:gmem=42G:j_exclusive=no:gpack=yes
    # If job>43GB, ask for exclusive GPU use
    # this MUST be limited to 2 exclusive use jobs per user!
    # gpu: num=1:mode=exclusive_process:gmem=47G:j_exclusive=yes

    # Which machine to use:
    #  - lsf-server-2 = CD/big
    #  - lsf-server-3 = titanx
    # To use any, uncomment!
    machines: lsf-server-2


## Uncommment the following to schedule a job to start at a specific time
## In the following case, the job will be scheduled at 8PM of the day you submit it
## The format is YYYY:MM:DD:HH:M, https://www.ibm.com/support/knowledgecenter/en/SSWRJV_10.1.0/lsf_command_ref/bsub.b.1.html
# schedule:
#    specifiedStartTime: 20:00


## Uncomment the following to get job status updates on your @rostlab.org email
## Note: the default behaviour is that the @rostlab.org email forwards to your TUM email
#notify:
#    notifyJobDone: ""
#    notifyJobExit:
#    jobWaitDone:
#    notifyJobDispatch:


properties:
    queueName: mid-end-normal
    jobName: train_with_esm_embeddings
#command: python train_with_embeds.py --bs 40 --lr 0.0001 --epochs 20 --temb data/train_esm2_t12_35M_UR50D_embeddings.h5 --vemb data/val_esm2_t12_35M_UR50D_embeddings.h5 --wname 35M_embeds && python train_with_embeds.py --bs 40 --lr 0.0001 --epochs 20 --temb data/train_esm2_t6_8M_UR50D_embeddings.h5 --vemb data/val_esm2_t6_8M_UR50D_embeddings.h5 --wname 8M_embeds
#command: python make_esm_embeddings.py data/train.jsonl -1 data/ && python make_esm_embeddings.py data/val.jsonl -1 data/ && python make_esm_embeddings.py data/train_filter_no256.jsonl -1 data/
# command: python train_with_embeds.py --bs 40 --lr 0.0001 --epochs 20 --temb data/train_esm2_t33_650M_UR50D_embeddings.h5 --vemb data/val_esm2_t33_650M_UR50D_embeddings.h5 --wname 650M
command:
    python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_esm2_t6_8M_UR50D_embeddings.h5 --vemb data/embeddings/val_esm2_t6_8M_UR50D_embeddings.h5 --casp12_embeds data/embeddings/casp12_esm2_t6_8M_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/casp12_esm2_t6_8M_UR50D_embeddings.h5 --train_labels data/train.jsonl --val_labels data/val.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl &&
    python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_esm2_t12_35M_UR50D_embeddings.h5 --vemb data/embeddings/val_esm2_t12_35M_UR50D_embeddings.h5 --casp12_embeds data/embeddings/casp12_esm2_t12_35M_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/new_pisces_esm2_t12_35M_UR50D_embeddings.h5 --train_labels data/train.jsonl --val_labels data/val.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl &&
    python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_esm2_t30_150M_UR50D_embeddings.h5 --vemb data/embeddings/val_esm2_t30_150M_UR50D_embeddings.h5 --casp12_embeds data/embeddings/casp12_esm2_t30_150M_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/new_pisces_esm2_t30_150M_UR50D_embeddings.h5 --train_labels data/train.jsonl --val_labels data/val.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl &&
    python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_esm2_t33_650M_UR50D_embeddings.h5 --vemb data/embeddings/val_esm2_t33_650M_UR50D_embeddings.h5 --casp12_embeds data/embeddings/casp12_esm2_t33_650M_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/new_pisces_esm2_t33_650M_UR50D_embeddings.h5 --train_labels data/train.jsonl --val_labels data/val.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl &&
#python --lmt esm --epochs 16 --bs 40 --temb data/embeddings/train_esm2_t36_3B_UR50D_embeddings.h5 --vemb data/embeddings/val_esm2_t36_3B_UR50D_embeddings.h5 --casp12_embeds data/embeddings/casp12_esm2_t36_3B_UR50D_embeddings.h5 --new_pisces_embeds data/embeddings/new_pisces_esm2_t36_3B_UR50D_embeddings.h5 --train_labels data/train.jsonl --val_labels data/val.jsonl --model_out_dir models/ --casp12_labels data/casp12.jsonl --new_pisces_labels data/new_pisces.jsonl
